### Lesson 1
- Transfer Learning on Cats vs Dogs breed classification problem with resnet34(5.7% error) and resnet50(4.4% error).
- Set the Learning rate using Leslie Smith's learning rate finder method.
- Language models zoo - https://dawn.cs.stanford.edu/benchmark/
- Labelling dataset.
#### Papers
1. - Oxford-IIT pet dataset - http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf
   - Dataset statistics - http://www.robots.ox.ac.uk/~vgg/data/pets/
   
2. Visualizing and Understanding CNN - https://arxiv.org/pdf/1311.2901.pdf
3. Learning Rate finder - Cyclical Learning rate - https://arxiv.org/pdf/1506.01186.pdf
4. One Cycle policy fitting paper - https://arxiv.org/pdf/1803.09820.pdf

### Lesson 2
- Download the images data from google - https://images.google.com/
- Deploy models
- SGD from scratch

#### Papers
1. 



### Lesson 3
- 


#### Papers
1. 


### Lesson 4
- Language Models
- ULMFit for sentiment analysis
- Tabular data - Adult dataset
- Collaborative filtering using movielens dataset
- cold start problem
- Demonstartion of collaborative filtering using excel sheet.

#### Papers
1. ULMFit - https://arxiv.org/pdf/1801.06146.pdf


### Lesson 5
- Explanation of cnn_learner(), fit, discriminative lr in fastai.
- Explanation of embeddings in collaborative filtering
- Explanation of optimization algorithms in Excel sheet.
- softmax and CrossEntropyLoss details.

#### Links
1. Collaborative filtering in Netflix - https://towardsdatascience.com/netflix-and-chill-building-a-recommendation-system-in-excel-c69b33c914f4
2. GD techniques by sebasian ruder - https://ruder.io/optimizing-gradient-descent/


### Lesson 6
- Platform.ai is used to label images in a easy way.
- Tabluar data (Regression) - Rossmann store sales data
- Dropout , Batch Normalization, data augmentation
- convolutions explanation.

#### Links
1. platform.ai - https://platform.ai/ 
2. Dropout paper - https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf
3. Batch Normalization paper - https://arxiv.org/pdf/1502.03167.pdf
4. How Convolutions work - https://brohrer.github.io/how_convolutional_neural_networks_work.html
5. Classification loss diff - https://gombru.github.io/2018/05/23/cross_entropy_loss/
6. BERT LM - https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270

### Lesson 7
- Resnet with MNIST , custom CNN layers.
- food classifier - https://apps.apple.com/us/app/food-classifier/id1445356461
- Explanation of Resnets , DenseNets, UNets.
- Image Generation tasks.
- Low res to high res images using GANs.
- low res to high res images with WGAN - LSUN bedroom dataset.
- superres notebook with vgg16 using perceptual losses.
- Explanation of RNNs with human numbers notebook.

#### Links
1. Resnets paper - https://arxiv.org/pdf/1512.03385.pdf
2. Visualizing Loss paper - https://arxiv.org/pdf/1712.09913.pdf
3. DenseNets - https://arxiv.org/pdf/1608.06993.pdf
4. UNets - https://arxiv.org/pdf/1505.04597.pdf
5. Perceptual Losses - https://arxiv.org/pdf/1603.08155.pdf


### Lesson 8
- Overview of the course
- Swift , Julia and halide languages
- How to build a library - 00_exports notebook
- Matmul notebook - forbenius norm, elementwise matmul, broadcasting, Einstein summation, pytorch matmul()
- cublas, mkl - GPU languages
- 02_fullyconnected notebook - initialization explanationn, forward , backward pass.


#### Links
1. Detexify - https://detexify.kirelabs.org/classify.html
2. FIXUP INITIALIZATION: RESIDUAL LEARNING WITHOUT NORMALIZATION - https://arxiv.org/pdf/1901.09321.pdf
3. He initialization - https://arxiv.org/pdf/1502.01852.pdf
4. Xavier initialization - http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
5. Matrix calculation you need for DL - https://arxiv.org/pdf/1802.01528.pdf




Ten techniques from fastai - https://blog.floydhub.com/ten-techniques-from-fast-ai/
